{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb57c35f-1e93-4d31-b388-8c17a63902bc",
   "metadata": {},
   "source": [
    "# Boosting Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307ecf2-95f7-4a80-8fd4-ef1073021438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edab5d12-2e7b-4e57-8be5-efdcc3ff06c0",
   "metadata": {},
   "source": [
    "Q 1 ANS:-\n",
    "\n",
    "Gradient Boosting Regression is a popular machine learning algorithm that belongs to the family of boosting methods. It is primarily used for regression tasks, where the goal is to predict a continuous numerical value. Gradient Boosting Regression combines multiple weak regression models (often decision trees) into a powerful ensemble model.\n",
    "\n",
    "Here's a high-level overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. Initialization: Initially, the ensemble model is initialized with a constant value, typically the mean or median of the target variable.\n",
    "\n",
    "2. Iterative Training: The algorithm performs a series of iterations, with each iteration aiming to improve the ensemble's performance.\n",
    "\n",
    "3. Building Weak Regression Models: In each iteration, a weak regression model, typically a decision tree with limited depth, is trained on the current residuals (the differences between the predicted values and the actual values).\n",
    "\n",
    "4. Calculating the Pseudo-Residuals: The pseudo-residuals for each training instance are calculated based on the difference between the actual target values and the predictions made by the current ensemble model.\n",
    "\n",
    "5. Training Weak Model on Pseudo-Residuals: The weak regression model is trained to predict the pseudo-residuals instead of the target variable. The goal is to find the best split points in the features that can minimize the loss when predicting the pseudo-residuals.\n",
    "\n",
    "6. Updating Ensemble Predictions: The predictions of the weak model are added to the ensemble's predictions, gradually improving the overall prediction accuracy.\n",
    "\n",
    "7. Learning Rate: Each weak model's contribution to the ensemble is scaled by a learning rate parameter, which controls the step size at which the ensemble learns from the weak models. A lower learning rate can help prevent overfitting and lead to a more robust and accurate model.\n",
    "\n",
    "8. Iteration Termination: The iterations continue until a specified number of weak models are trained or until a predefined stopping criterion is met. Common stopping criteria include reaching a maximum number of iterations, achieving a desired level of performance improvement, or when further iterations do not significantly improve the model.\n",
    "\n",
    "9. Final Ensemble: The final prediction of the Gradient Boosting Regression model is obtained by summing the predictions from all the weak models, weighted by the learning rate.\n",
    "\n",
    "Gradient Boosting Regression is effective in capturing complex nonlinear relationships in the data and handling a wide range of regression problems. It handles outliers and noisy data well and generally achieves high predictive accuracy. However, it may be more computationally expensive and prone to overfitting if not properly regularized or tuned. Regularization techniques, such as limiting the tree depth, subsampling, and early stopping, can help mitigate overfitting and improve generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51679e-ccf1-4c04-b84f-19f7762ac4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbd3165f-2125-433c-bd9b-983afb1809fb",
   "metadata": {},
   "source": [
    "Q 2 ANS:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b722124f-b317-4f07-88d9-cd92a0a50c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9417eaa-0765-4ff0-adc5-7757d84cf5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09d51eaf-7d44-4766-9f16-cbfe1c6280f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "912defc3-4717-44e8-a723-0e27f62815aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the ensemble with the mean value\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.estimators.append(initial_prediction)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute pseudo-residuals based on the current ensemble predictions\n",
    "            pseudo_residuals = y - self.predict(X)\n",
    "            \n",
    "            # Train a weak regression model on the pseudo-residuals\n",
    "            weak_model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            weak_model.fit(X, pseudo_residuals)\n",
    "            \n",
    "            # Update the ensemble by adding the predictions of the weak model, weighted by the learning rate\n",
    "            self.estimators.append(weak_model)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # Initialize the predictions with the mean value\n",
    "        predictions = np.full(X.shape[0], np.mean(self.estimators[0]))\n",
    "        \n",
    "        for estimator in self.estimators[1:]:\n",
    "            predictions += self.learning_rate * estimator.predict(X)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def mse(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def r_squared(self, y_true, y_pred):\n",
    "        ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "        return 1 - (ss_residual / ss_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3980c5c0-77fb-48dd-9ca8-2a929cd0ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad1d6c30-61bf-4785-ad00-3f280eb4f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a527070-e16f-4eef-aed5-709a5f7e5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = gb_model.mse(y_test, y_pred)\n",
    "r2 = gb_model.r_squared(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66706414-c9c6-42e9-891f-f6cc4b84efe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3379888778506104\n",
      "R-squared: 0.9990403356176427\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be29e5c9-f4fb-445d-a50c-b1bd1d3ce60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6324e61e-035c-4c1a-b011-2176b284c464",
   "metadata": {},
   "source": [
    "Q 3 ANS:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85a7242e-5cd2-4336-bac2-a4a2a0c62c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96f08fce-fbfe-42af-89a9-a60a79ce2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "parameter = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35916a9a-ea1e-4306-98f6-120d004e0954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the gradient boosting regressor\n",
    "gb_model = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "214c6d73-24e1-4671-9b4d-4bfa588432a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.5],\n",
       "                         &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.5],\n",
       "                         &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200]},\n",
       "             scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.5],\n",
       "                         'max_depth': [3, 5, 7],\n",
       "                         'n_estimators': [50, 100, 200]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_model, param_grid=parameter, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "088d78e7-ade5-4819-bb65-2d108dfe2be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the best hyperparameters found\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d43fa50d-f225-4bc7-8056-8f7b56bd309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c467368c-30d4-4dd3-a5c7-661a2d90dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e8445da-6426-4895-9115-7eac09b1af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.3379888778506104\n",
      "R-squared: 0.9990403356176427\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f8754-bfea-499d-8181-397e1b699c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a808f382-b933-44c5-80d8-a779e1e6df03",
   "metadata": {},
   "source": [
    "Q 4 ANS:-\n",
    "\n",
    "In Gradient Boosting, a \"weak learner\" refers to a simple, relatively low-complexity model that performs only slightly better than random guessing on a given learning task. Weak learners are the building blocks used in the boosting algorithm to construct a strong ensemble model.\n",
    "\n",
    "Some common examples of weak learners are:\n",
    "\n",
    "1. Decision Stumps: A decision stump is a decision tree with a single split. It has a depth of one and makes predictions based on a single feature's value.\n",
    "\n",
    "2. Shallow Decision Trees: These are decision trees with limited depth (e.g., depth 2 or 3). They are less likely to overfit the training data but are still considered weak as they have limited expressiveness.\n",
    "\n",
    "3. Linear Models: Simple linear regression or logistic regression models can also serve as weak learners, especially in the context of Gradient Boosting for regression or classification tasks.\n",
    "\n",
    "4. K-Nearest Neighbors with K = 1: Using a single nearest neighbor for classification can be considered a weak learner, as it is sensitive to noise in the data.\n",
    "\n",
    "5. Perceptron: A single-layer neural network with a linear activation function can also act as a weak learner.\n",
    "\n",
    "The key characteristic of a weak learner is that it should perform slightly better than random guessing on the training data. When combined in an ensemble using the boosting algorithm, weak learners are trained sequentially, and each subsequent weak learner focuses on the mistakes made by the previous ones. This allows the ensemble to progressively improve its performance and build a strong model that is capable of making accurate predictions.\n",
    "\n",
    "By combining multiple weak learners, Gradient Boosting leverages their individual strengths and compensates for their weaknesses, resulting in a powerful and flexible model capable of capturing complex patterns in the data. The iterative nature of the boosting algorithm, focusing on difficult instances in each iteration, enables the final ensemble model to achieve high predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c79e54-5454-4612-9839-22820a3915a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49444c43-4f25-478c-a016-fa9acaa147ad",
   "metadata": {},
   "source": [
    "Q 5 ANS:-\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1. Iterative Improvement: Gradient Boosting is an iterative algorithm that builds an ensemble of weak learners in a sequential manner. Each weak learner is trained to correct the mistakes made by the previous ones. In each iteration, the algorithm focuses on the instances that were not accurately predicted by the ensemble so far.\n",
    "\n",
    "2. Residual Learning: Instead of trying to directly fit the target variable, Gradient Boosting aims to fit the residual errors (the differences between the actual values and the predictions made by the current ensemble). Each weak learner is trained to predict the residuals or gradients of the loss function.\n",
    "\n",
    "3. Gradient Descent Optimization: Gradient Boosting uses gradient descent optimization to find the best direction for updating the ensemble model. It calculates the negative gradient of the loss function with respect to the ensemble's predictions and uses this information to update the ensemble.\n",
    "\n",
    "4. Weighted Contribution: Each weak learner is given a weight based on its performance. Weak learners that contribute more to reducing the loss are assigned higher weights, while those that contribute less are assigned lower weights. This allows the algorithm to focus more on the weak learners that are better at capturing the patterns in the data.\n",
    "\n",
    "5. Combining Weak Learners: The final ensemble model is created by combining the predictions of all the weak learners, typically using a weighted sum. The weights assigned to the weak learners reflect their individual contributions to the overall prediction.\n",
    "\n",
    "6. Regularization: Gradient Boosting includes regularization techniques to prevent overfitting and control the complexity of the ensemble model. Regularization can be achieved through parameters like learning rate, maximum depth of weak learners, and subsampling of the training data.\n",
    "\n",
    "The intuition behind Gradient Boosting is to iteratively build a strong model by combining the predictions of multiple weak learners. Each weak learner focuses on the mistakes made by the previous ones, gradually improving the model's predictive accuracy. By leveraging the gradients of the loss function, the algorithm optimizes the ensemble's predictions in the direction of steepest descent. The weighted contribution of weak learners and regularization techniques ensure a robust and accurate model that generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd3518d-dbcc-4eae-9f02-4463232988da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "037f393e-d6b5-4eff-8037-a9e9c24c64c7",
   "metadata": {},
   "source": [
    "Q 6 ANS:-\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's a step-by-step explanation of how the ensemble is constructed:\n",
    "\n",
    "1. Initialization: Initially, the ensemble is initialized with a constant value, typically the mean or median of the target variable. This serves as the starting point for subsequent iterations.\n",
    "\n",
    "2. Calculate Pseudo-Residuals: In each iteration, the algorithm calculates the pseudo-residuals, which represent the differences between the actual target values and the predictions made by the current ensemble. The pseudo-residuals capture the mistakes or errors made by the ensemble so far.\n",
    "\n",
    "3. Train a Weak Learner: A weak learner, such as a decision tree with limited depth, is trained to predict the pseudo-residuals. The weak learner is trained to minimize the loss function with respect to the pseudo-residuals. The goal is to find the best split points in the features that can minimize the loss when predicting the pseudo-residuals.\n",
    "\n",
    "4. Update Ensemble Predictions: The predictions of the weak learner are added to the ensemble's predictions. However, to prevent overfitting and control the contribution of each weak learner, a learning rate parameter is introduced. The learning rate scales the contribution of each weak learner, allowing for a controlled step towards the optimal direction.\n",
    "\n",
    "5. Iterate and Repeat: Steps 2-4 are repeated for a specified number of iterations or until a stopping criterion is met. At each iteration, a new weak learner is trained to predict the pseudo-residuals, and its predictions are added to the ensemble's predictions, gradually improving the overall prediction accuracy.\n",
    "\n",
    "6. Final Ensemble Prediction: The final prediction of the Gradient Boosting algorithm is obtained by summing the predictions from all the weak learners in the ensemble, weighted by the learning rate. The ensemble prediction is the combination of all the weak learners' predictions, which collectively form a stronger model than any individual weak learner.\n",
    "\n",
    "The process of building an ensemble of weak learners in Gradient Boosting leverages the principle of \"boosting,\" where each weak learner focuses on correcting the mistakes of the ensemble so far. By iteratively training weak learners and updating the ensemble predictions, the algorithm progressively improves the overall performance of the model, capturing complex patterns and interactions in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc621dc-639e-4def-b05d-f70fdb609026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae168855-2d08-4ccf-9709-e32badfb5f24",
   "metadata": {},
   "source": [
    "Q 7 ANS:-\n",
    "\n",
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1. Define a Loss Function: The first step is to define a differentiable loss function that measures the discrepancy between the actual target values and the predictions made by the model. Common loss functions include mean squared error (MSE) for regression tasks and log loss or exponential loss for classification tasks.\n",
    "\n",
    "2. Initialize the Ensemble: The ensemble is initialized with a constant value, typically the mean or median of the target variable. This serves as the starting point for subsequent iterations.\n",
    "\n",
    "3. Calculate Pseudo-Residuals: Pseudo-residuals are calculated by taking the negative gradient of the loss function with respect to the current ensemble's predictions. The pseudo-residuals capture the mistakes or errors made by the ensemble so far and represent the direction in which the ensemble's predictions need to be corrected.\n",
    "\n",
    "4. Train a Weak Learner: A weak learner, often a decision tree with limited depth, is trained to predict the pseudo-residuals. The weak learner is trained to minimize the loss function with respect to the pseudo-residuals. The goal is to find the best split points in the features that can minimize the loss when predicting the pseudo-residuals.\n",
    "\n",
    "5. Update Ensemble Predictions: The predictions of the weak learner are added to the ensemble's predictions, weighted by a learning rate parameter. The learning rate controls the contribution of each weak learner and prevents overfitting. By multiplying the predictions of the weak learner by the learning rate, the impact of each weak learner on the ensemble is regulated.\n",
    "\n",
    "6. Iterate and Repeat: Steps 3-5 are repeated for a specified number of iterations or until a stopping criterion is met. In each iteration, new weak learners are trained to predict the pseudo-residuals, and their predictions are added to the ensemble's predictions. The ensemble gradually improves its predictive performance by iteratively correcting the errors made in previous iterations.\n",
    "\n",
    "7. Final Ensemble Prediction: The final prediction of the Gradient Boosting algorithm is obtained by summing the predictions from all the weak learners in the ensemble, weighted by the learning rate. The ensemble prediction is the combination of all the weak learners' predictions, which collectively form a stronger model than any individual weak learner.\n",
    "\n",
    "The mathematical intuition of Gradient Boosting lies in minimizing the loss function by iteratively fitting weak learners to the negative gradients of the loss function. Each weak learner focuses on the errors made by the ensemble so far, and their predictions are added to the ensemble, progressively reducing the overall loss. By combining the predictions of multiple weak learners, the algorithm constructs a powerful ensemble model capable of capturing complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df4623-4cf0-44f8-bb9b-ba6d78bbae2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
